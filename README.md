# Deep Reinforcement Learning â€“ MSc Course (LIACS)

This repository contains all code, experiments, reports, and analysis from my work on the Deep Reinforcement Learning (DRL) course at Leiden University.

## ğŸ“ Structure

```bash
DEEP-RL/
â”‚
â”œâ”€â”€ assignment-1/               # Assignment 1: Q-Learning & Deep Q-Networks
â”‚   â”œâ”€â”€ naive-dqn/              # Naive DQN implementation
â”‚   â”œâ”€â”€ stable-dqn/             # Target networks, experience replay
â”‚   â”œâ”€â”€ bonus/                  # Optional enhancements
â”‚   â”œâ”€â”€ visuals/                # Plots and visualizations
â”‚   â”œâ”€â”€ final_report.pdf        # PDF report for A1
â”‚   â”œâ”€â”€ plot_all.py             # Plotting script
â”‚   â””â”€â”€ run_ablation_studies.sh
â”‚
â”œâ”€â”€ assignment-2/               # Assignment 2: Policy Gradient Algorithms
â”‚   â”œâ”€â”€ data/                   # Experiment logs (CSV)
â”‚   â”œâ”€â”€ plots/                  # Generated graphs
â”‚   â”œâ”€â”€ report/                 # LaTeX source for ICML-style report
â”‚   â”œâ”€â”€ hyperparameter-tuning/ # Ablation scripts
â”‚   â”œâ”€â”€ submission/             # Clean submission copy
â”‚   â”œâ”€â”€ reinforce.py            # REINFORCE implementation
â”‚   â”œâ”€â”€ ac.py                   # Actor-Critic (vanilla)
â”‚   â”œâ”€â”€ a2c.py                  # Advantage Actor-Critic (A2C)
â”‚   â”œâ”€â”€ models.py               # Shared model architectures
â”‚   â”œâ”€â”€ plot_all.py             # Plotting script (CartPole)
â”‚   â”œâ”€â”€ plot_all_bonus.py       # Plotting script (Acrobot)
â”‚   â””â”€â”€ run_all.sh              # Run all experiments
â”‚
â””â”€â”€ README.md                   # You are here
```

---

## ğŸ§  Assignment Overview

### ğŸŸ¢ **Assignment 1 â€“ Value-Based Methods**
- Implemented tabular Q-learning and Deep Q-Networks (DQN).
- Added experience replay and target networks.
- Compared naive vs stabilized DQN setups.

### ğŸ”µ **Assignment 2 â€“ Policy-Based Methods**
- Implemented REINFORCE, Actor-Critic, and Advantage Actor-Critic (A2C).
- Trained and evaluated on `CartPole-v1` and `Acrobot-v1`.
- Performed ablation studies on learning rate and critic capacity.
- Included comparison with DQN for reference.

---

## âš™ï¸ Running Experiments

### Setup Environment

```bash
python -m venv drl-env
source drl-env/bin/activate  # Windows: drl-env\Scripts\activate
```

### Install Requirements

Each assignment folder has its own `requirements.txt`:

```bash
cd assignment-2/
pip install -r requirements.txt
```

### Run Experiments

Each assignment has a single run script to execute all experiments:

```bash
# Assignment 1
cd assignment-1/
bash run_ablation_studies.sh

# Assignment 2
cd assignment-2/
bash run_all.sh
```

---

## ğŸ“Š Visualization

After running experiments, results are saved in `data/` and `plots/`. Use the included plotting scripts to regenerate all comparison graphs:

```bash
python plot_all.py           # For CartPole
python plot_all_bonus.py     # For Acrobot
```

---

## ğŸ“„ Reports

Each assignment includes a final PDF write-up following the ICML LaTeX template. Reports include:
- Background theory
- Implementation details
- Experiment setup
- Results and analysis
- Visualizations

---

## ğŸ“Œ Resources

- [Gymnasium (Farama)](https://gymnasium.farama.org/)
- [Sutton & Barto â€“ RL Book](http://incompleteideas.net/book/the-book.html)
- [Spinning Up â€“ OpenAI](https://spinningup.openai.com/en/latest/)

---

## ğŸ‘‹ Contact

Feel free to reach out if you'd like to discuss implementation details or results. This repository is maintained as part of my MSc Data Science coursework.

